{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f38777-bf6e-440f-8714-ab491b701675",
   "metadata": {},
   "source": [
    "# Tutorial 6: ResNet-18 and Image Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the Python Tutorial on ResNet-18 and Image Classification! In this comprehensive guide, we will explore one of the most influential deep learning architectures for image recognition tasks - ResNet-18. ResNet (Residual Network) is a type of convolutional neural network (CNN) that introduced the concept of residual blocks to address the vanishing gradient problem in very deep networks.\n",
    "\n",
    "ResNet-18 is a specific variant of the ResNet architecture with 18 layers. It has achieved remarkable success in various image classification competitions and real-world applications. In this tutorial, we will walk you through the implementation of ResNet-18 using Python and popular deep learning libraries, enabling you to harness the power of this architecture for your own image classification tasks.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before diving into this tutorial, it is recommended to have a solid understanding of the following topics:\n",
    "\n",
    "- Python programming fundamentals\n",
    "- Basics of machine learning and deep learning\n",
    "- Convolutional Neural Networks (CNNs) - Understanding their working will be beneficial for grasping ResNet-18's concepts.\n",
    "\n",
    "Knowledge of libraries like NumPy, PyTorch (or TensorFlow), and Matplotlib will be helpful, as we will use them extensively in our implementations and visualizations.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "- Understand the fundamentals of the ResNet architecture, particularly ResNet-18.\n",
    "- Comprehend the concept of residual blocks and their role in addressing the vanishing gradient problem.\n",
    "- Implement ResNet-18 in Python using PyTorch, a popular deep learning framework.\n",
    "- Train and fine-tune the ResNet-18 model on a dataset for image classification.\n",
    "- Evaluate the model's performance and make predictions on new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d491e-39b9-4ee8-9199-29a01ef26419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "def model_A(num_classes):\n",
    "    # pretrained = True means we use the pretrained parameters of ResNet18\n",
    "    model_resnet = models.resnet18(pretrained=True)\n",
    "    num_features = model_resnet.fc.in_features # The input channels of the full connection layer\n",
    "    model_resnet.fc = nn.Linear(num_features, num_classes) # We modify the number of classes\n",
    "    # We only train the full connection layer (fine-tune)\n",
    "    for param in model_resnet.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model_resnet.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    return model_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca5a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "## Note that: here we provide a basic solution for loading data and transforming data.\n",
    "## You can directly change it if you find something wrong or not good enough.\n",
    "\n",
    "## the mean and standard variance of imagenet dataset\n",
    "## mean_vals = [0.485, 0.456, 0.406]\n",
    "## std_vals = [0.229, 0.224, 0.225]\n",
    "\n",
    "def load_data(data_dir = \"./data/\",input_size = 224,batch_size = 36):\n",
    "    # data augmentation\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(input_size), # Resize to 224 * 224\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    ## Load dataset\n",
    "    ## For other tasks, you may need to modify the data dir or even rewrite some part of 'data.py'\n",
    "    image_dataset_train = datasets.ImageFolder(os.path.join(data_dir, '2-Medium-Scale'), data_transforms['train'])\n",
    "    image_dataset_valid = datasets.ImageFolder(os.path.join(data_dir, 'test'), data_transforms['test'])\n",
    "\n",
    "    train_loader = DataLoader(image_dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    valid_loader = DataLoader(image_dataset_valid, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b790500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "## Note that: here we provide a basic solution for training and validation.\n",
    "## You can directly change it if you find something wrong or not good enough.\n",
    "\n",
    "def train_model(model,train_loader, valid_loader, criterion, optimizer, num_epochs=20):\n",
    "\n",
    "    def train(model, train_loader,optimizer,criterion):\n",
    "        model.train(True)\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            # send the data to device (GPU)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs) # prediction\n",
    "            loss = criterion(outputs, labels) # loss\n",
    "            _, predictions = torch.max(outputs, 1) # The class with maximal probability\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_correct += torch.sum(predictions == labels.data)\n",
    "\n",
    "        epoch_loss = total_loss / len(train_loader.dataset)\n",
    "        epoch_acc = total_correct.double() / len(train_loader.dataset)\n",
    "        return epoch_loss, epoch_acc.item()\n",
    "\n",
    "    def valid(model, valid_loader,criterion):\n",
    "        model.train(False)\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_correct += torch.sum(predictions == labels.data)\n",
    "        epoch_loss = total_loss / len(valid_loader.dataset)\n",
    "        epoch_acc = total_correct.double() / len(valid_loader.dataset)\n",
    "        return epoch_loss, epoch_acc.item()\n",
    "\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('*' * 100)\n",
    "        print('epoch:{:d}/{:d}'.format(epoch, num_epochs))\n",
    "        train_loss, train_acc = train(model, train_loader,optimizer,criterion)\n",
    "        print(\"training: loss:   {:.4f}, accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "        valid_loss, valid_acc = valid(model, valid_loader,criterion)\n",
    "        print(\"validation: loss: {:.4f}, accuracy: {:.4f}\".format(valid_loss, valid_acc))\n",
    "        # save the best model\n",
    "        if valid_acc > best_acc:\n",
    "            best_acc = valid_acc\n",
    "            best_model = model\n",
    "            torch.save(best_model, 'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ee6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "## about model\n",
    "num_classes = 10\n",
    "\n",
    "## about data\n",
    "data_dir = \"data\" ## You may need to specify the data_dir first\n",
    "inupt_size = 224\n",
    "batch_size = 18\n",
    "\n",
    "## about training\n",
    "num_epochs = 20\n",
    "lr = 0.001\n",
    "\n",
    "## model initialization\n",
    "model = model_A(num_classes=num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:', device)\n",
    "model = model.to(device)\n",
    "\n",
    "## optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "## loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "## data preparation\n",
    "train_loader, valid_loader = load_data(data_dir=data_dir,input_size=inupt_size, batch_size=batch_size)\n",
    "# train\n",
    "train_model(model,train_loader, valid_loader, criterion, optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1debc41d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
