{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cdb54ce-6b4f-423e-971a-faf57a8f5aef",
   "metadata": {},
   "source": [
    "# Tutorial 6: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the Python Tutorial on Convolutional Neural Networks (CNNs)! In this comprehensive guide, we will dive into one of the most important and powerful deep learning architectures for image processing - CNNs. CNNs are designed to automatically and adaptively learn spatial hierarchies of features from input images.\n",
    "\n",
    "CNNs have revolutionized computer vision tasks, such as image classification, object detection, segmentation, and more. In this tutorial, we will explore the fundamental concepts of CNNs, their architecture, and the mathematics behind their working. We will also walk through the implementation of CNNs in Python using popular deep learning libraries.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before diving into this tutorial, it is recommended to have a solid understanding of the following topics:\n",
    "\n",
    "- Python programming fundamentals\n",
    "- Basics of machine learning and neural networks\n",
    "- Linear algebra and calculus concepts - Understanding matrices, vectors, and derivatives will be beneficial for grasping CNNs.\n",
    "\n",
    "Knowledge of libraries like NumPy, PyTorch, and Matplotlib will be helpful, as we will use them properly in our implementations and visualizations.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "- Understand the fundamental building blocks of Convolutional Neural Networks (CNNs).\n",
    "- Comprehend the concept of convolution and its role in learning local patterns.\n",
    "- Implement a basic CNN in Python using PyTorch, a popular deep learning framework.\n",
    "- Train the CNN model on a dataset for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc947af0-a9e7-4eb2-837a-3a29b837cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a7d3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, classes = 10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, classes)\n",
    "       \n",
    "\n",
    "    def forward(self, x):  # input: batch_size * 3 * 64 * 64\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = self.conv1(x) # batch_size * 32 * 64 * 64\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)  # batch_size * 32 * 32 * 32\n",
    "        \n",
    "        x = self.conv2(x) # batch_size * 64 * 32 * 32\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)  # batch_size * 64 * 16 * 16\n",
    "        \n",
    "        x = self.conv3(x) # batch_size * 128 * 16 * 16\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)  # batch_size * 128 * 8 * 8\n",
    "        \n",
    "        x = x.view(batch_size, -1) # batch_size * (128 * 8 * 8)\n",
    "        x = self.fc1(x)            # batch_size * 10\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cca5a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "## the mean and standard variance of imagenet dataset\n",
    "## mean_vals = [0.485, 0.456, 0.406]\n",
    "## std_vals = [0.229, 0.224, 0.225]\n",
    "\n",
    "def load_data(data_dir = \"./data/\",input_size = 64,batch_size = 36):\n",
    "    # data augmentation\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    ## Load dataset\n",
    "    ## For other tasks, you may need to modify the data dir or even rewrite some part of 'data.py'\n",
    "    image_dataset_train = datasets.ImageFolder(os.path.join(data_dir, '2-Medium-Scale'), data_transforms['train'])\n",
    "    image_dataset_valid = datasets.ImageFolder(os.path.join(data_dir, 'test'), data_transforms['test'])\n",
    "\n",
    "    train_loader = DataLoader(image_dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    valid_loader = DataLoader(image_dataset_valid, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b790500",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that: here we provide a basic solution for training and validation.\n",
    "## You can directly change it if you find something wrong or not good enough.\n",
    "\n",
    "def train_model(model,train_loader, valid_loader, criterion, optimizer, num_epochs=20):        \n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # train the model\n",
    "        model.train(True)\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            # send the data to device (GPU)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs) # prediction\n",
    "            loss = criterion(outputs, labels) # loss\n",
    "            _, predictions = torch.max(outputs, 1) # The class with maximal probability\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_correct += torch.sum(predictions == labels.data)\n",
    "        train_loss = total_loss / len(train_loader.dataset)\n",
    "        train_acc = total_correct.double() / len(train_loader.dataset)\n",
    "        \n",
    "        # test\n",
    "        model.train(False)\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_correct += torch.sum(predictions == labels.data)\n",
    "        valid_loss = total_loss / len(valid_loader.dataset)\n",
    "        valid_acc = total_correct.double() / len(valid_loader.dataset)\n",
    "        \n",
    "        # Show the results\n",
    "        print('*' * 100)\n",
    "        print('epoch:{:d}/{:d}'.format(epoch, num_epochs))\n",
    "        print(\"training: loss:   {:.4f}, accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "        print(\"validation: loss: {:.4f}, accuracy: {:.4f}\".format(valid_loss, valid_acc))\n",
    "        \n",
    "        # save the best model\n",
    "        if valid_acc > best_acc:\n",
    "            best_acc = valid_acc\n",
    "            best_model = model\n",
    "            torch.save(best_model, 'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a72ee6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "****************************************************************************************************\n",
      "epoch:0/50\n",
      "training: loss:   6.4479, accuracy: 0.1480\n",
      "validation: loss: 7.7156, accuracy: 0.2000\n",
      "****************************************************************************************************\n",
      "epoch:1/50\n",
      "training: loss:   5.1180, accuracy: 0.2820\n",
      "validation: loss: 10.2933, accuracy: 0.2700\n",
      "****************************************************************************************************\n",
      "epoch:2/50\n",
      "training: loss:   3.6913, accuracy: 0.3160\n",
      "validation: loss: 5.0701, accuracy: 0.2600\n",
      "****************************************************************************************************\n",
      "epoch:3/50\n",
      "training: loss:   3.0899, accuracy: 0.3180\n",
      "validation: loss: 3.8203, accuracy: 0.2800\n",
      "****************************************************************************************************\n",
      "epoch:4/50\n",
      "training: loss:   2.6755, accuracy: 0.3840\n",
      "validation: loss: 2.2697, accuracy: 0.3800\n",
      "****************************************************************************************************\n",
      "epoch:5/50\n",
      "training: loss:   2.1834, accuracy: 0.3920\n",
      "validation: loss: 2.1474, accuracy: 0.3900\n",
      "****************************************************************************************************\n",
      "epoch:6/50\n",
      "training: loss:   1.9609, accuracy: 0.4580\n",
      "validation: loss: 2.1636, accuracy: 0.4100\n",
      "****************************************************************************************************\n",
      "epoch:7/50\n",
      "training: loss:   1.6733, accuracy: 0.5380\n",
      "validation: loss: 1.4613, accuracy: 0.5100\n",
      "****************************************************************************************************\n",
      "epoch:8/50\n",
      "training: loss:   1.5791, accuracy: 0.5020\n",
      "validation: loss: 1.8864, accuracy: 0.4700\n",
      "****************************************************************************************************\n",
      "epoch:9/50\n",
      "training: loss:   1.6869, accuracy: 0.4740\n",
      "validation: loss: 2.0657, accuracy: 0.4400\n",
      "****************************************************************************************************\n",
      "epoch:10/50\n",
      "training: loss:   2.0562, accuracy: 0.4680\n",
      "validation: loss: 1.5794, accuracy: 0.5600\n",
      "****************************************************************************************************\n",
      "epoch:11/50\n",
      "training: loss:   1.6903, accuracy: 0.5100\n",
      "validation: loss: 1.7793, accuracy: 0.4100\n",
      "****************************************************************************************************\n",
      "epoch:12/50\n",
      "training: loss:   1.5217, accuracy: 0.5220\n",
      "validation: loss: 1.3248, accuracy: 0.5700\n",
      "****************************************************************************************************\n",
      "epoch:13/50\n",
      "training: loss:   1.4033, accuracy: 0.5400\n",
      "validation: loss: 1.4271, accuracy: 0.5700\n",
      "****************************************************************************************************\n",
      "epoch:14/50\n",
      "training: loss:   1.4338, accuracy: 0.5140\n",
      "validation: loss: 1.4898, accuracy: 0.4800\n",
      "****************************************************************************************************\n",
      "epoch:15/50\n",
      "training: loss:   1.2736, accuracy: 0.5580\n",
      "validation: loss: 2.2862, accuracy: 0.4900\n",
      "****************************************************************************************************\n",
      "epoch:16/50\n",
      "training: loss:   1.3026, accuracy: 0.5640\n",
      "validation: loss: 1.2563, accuracy: 0.5500\n",
      "****************************************************************************************************\n",
      "epoch:17/50\n",
      "training: loss:   1.1639, accuracy: 0.5760\n",
      "validation: loss: 1.2366, accuracy: 0.5800\n",
      "****************************************************************************************************\n",
      "epoch:18/50\n",
      "training: loss:   1.2194, accuracy: 0.5920\n",
      "validation: loss: 1.0950, accuracy: 0.6700\n",
      "****************************************************************************************************\n",
      "epoch:19/50\n",
      "training: loss:   1.0888, accuracy: 0.6260\n",
      "validation: loss: 1.3845, accuracy: 0.5400\n",
      "****************************************************************************************************\n",
      "epoch:20/50\n",
      "training: loss:   1.1277, accuracy: 0.6100\n",
      "validation: loss: 1.2611, accuracy: 0.6000\n",
      "****************************************************************************************************\n",
      "epoch:21/50\n",
      "training: loss:   1.0421, accuracy: 0.6580\n",
      "validation: loss: 1.0007, accuracy: 0.6700\n",
      "****************************************************************************************************\n",
      "epoch:22/50\n",
      "training: loss:   1.0192, accuracy: 0.6400\n",
      "validation: loss: 1.1555, accuracy: 0.6000\n",
      "****************************************************************************************************\n",
      "epoch:23/50\n",
      "training: loss:   1.0476, accuracy: 0.6400\n",
      "validation: loss: 1.4828, accuracy: 0.5600\n",
      "****************************************************************************************************\n",
      "epoch:24/50\n",
      "training: loss:   0.9482, accuracy: 0.6720\n",
      "validation: loss: 1.1487, accuracy: 0.5900\n",
      "****************************************************************************************************\n",
      "epoch:25/50\n",
      "training: loss:   1.0442, accuracy: 0.6320\n",
      "validation: loss: 1.1240, accuracy: 0.6300\n",
      "****************************************************************************************************\n",
      "epoch:26/50\n",
      "training: loss:   1.0979, accuracy: 0.6360\n",
      "validation: loss: 1.4370, accuracy: 0.5600\n",
      "****************************************************************************************************\n",
      "epoch:27/50\n",
      "training: loss:   1.0086, accuracy: 0.6720\n",
      "validation: loss: 1.2886, accuracy: 0.5700\n",
      "****************************************************************************************************\n",
      "epoch:28/50\n",
      "training: loss:   0.9608, accuracy: 0.6380\n",
      "validation: loss: 1.1931, accuracy: 0.5500\n",
      "****************************************************************************************************\n",
      "epoch:29/50\n",
      "training: loss:   1.0308, accuracy: 0.6360\n",
      "validation: loss: 1.1818, accuracy: 0.6100\n",
      "****************************************************************************************************\n",
      "epoch:30/50\n",
      "training: loss:   1.0672, accuracy: 0.6440\n",
      "validation: loss: 1.3000, accuracy: 0.6000\n",
      "****************************************************************************************************\n",
      "epoch:31/50\n",
      "training: loss:   0.9262, accuracy: 0.6880\n",
      "validation: loss: 1.0577, accuracy: 0.6300\n",
      "****************************************************************************************************\n",
      "epoch:32/50\n",
      "training: loss:   1.0131, accuracy: 0.6760\n",
      "validation: loss: 1.2925, accuracy: 0.5400\n",
      "****************************************************************************************************\n",
      "epoch:33/50\n",
      "training: loss:   0.8799, accuracy: 0.6780\n",
      "validation: loss: 1.0644, accuracy: 0.6300\n",
      "****************************************************************************************************\n",
      "epoch:34/50\n",
      "training: loss:   0.9599, accuracy: 0.6720\n",
      "validation: loss: 1.1584, accuracy: 0.5500\n",
      "****************************************************************************************************\n",
      "epoch:35/50\n",
      "training: loss:   0.9089, accuracy: 0.6960\n",
      "validation: loss: 1.2337, accuracy: 0.6000\n",
      "****************************************************************************************************\n",
      "epoch:36/50\n",
      "training: loss:   0.8730, accuracy: 0.6640\n",
      "validation: loss: 1.0968, accuracy: 0.6200\n",
      "****************************************************************************************************\n",
      "epoch:37/50\n",
      "training: loss:   0.8679, accuracy: 0.6720\n",
      "validation: loss: 1.0167, accuracy: 0.6400\n",
      "****************************************************************************************************\n",
      "epoch:38/50\n",
      "training: loss:   0.7890, accuracy: 0.7040\n",
      "validation: loss: 0.9320, accuracy: 0.6700\n",
      "****************************************************************************************************\n",
      "epoch:39/50\n",
      "training: loss:   0.7955, accuracy: 0.7220\n",
      "validation: loss: 1.1079, accuracy: 0.6000\n",
      "****************************************************************************************************\n",
      "epoch:40/50\n",
      "training: loss:   0.7339, accuracy: 0.7240\n",
      "validation: loss: 1.0596, accuracy: 0.6100\n",
      "****************************************************************************************************\n",
      "epoch:41/50\n",
      "training: loss:   0.7517, accuracy: 0.7320\n",
      "validation: loss: 1.0527, accuracy: 0.6100\n",
      "****************************************************************************************************\n",
      "epoch:42/50\n",
      "training: loss:   0.8716, accuracy: 0.6940\n",
      "validation: loss: 1.0408, accuracy: 0.6800\n",
      "****************************************************************************************************\n",
      "epoch:43/50\n",
      "training: loss:   0.8774, accuracy: 0.6800\n",
      "validation: loss: 1.1825, accuracy: 0.6200\n",
      "****************************************************************************************************\n",
      "epoch:44/50\n",
      "training: loss:   0.8932, accuracy: 0.6760\n",
      "validation: loss: 1.2076, accuracy: 0.5700\n",
      "****************************************************************************************************\n",
      "epoch:45/50\n",
      "training: loss:   0.9228, accuracy: 0.6660\n",
      "validation: loss: 1.3181, accuracy: 0.5700\n",
      "****************************************************************************************************\n",
      "epoch:46/50\n",
      "training: loss:   0.8282, accuracy: 0.7120\n",
      "validation: loss: 1.1696, accuracy: 0.6100\n",
      "****************************************************************************************************\n",
      "epoch:47/50\n",
      "training: loss:   0.7623, accuracy: 0.7400\n",
      "validation: loss: 1.0366, accuracy: 0.6600\n",
      "****************************************************************************************************\n",
      "epoch:48/50\n",
      "training: loss:   0.8574, accuracy: 0.7080\n",
      "validation: loss: 1.4330, accuracy: 0.5700\n",
      "****************************************************************************************************\n",
      "epoch:49/50\n",
      "training: loss:   0.7828, accuracy: 0.7140\n",
      "validation: loss: 0.9395, accuracy: 0.6600\n"
     ]
    }
   ],
   "source": [
    "## about model\n",
    "num_classes = 10\n",
    "\n",
    "## about data\n",
    "data_dir = \"data\" ## You may need to specify the data_dir first\n",
    "inupt_size = 64\n",
    "batch_size = 64\n",
    "\n",
    "## about training\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "\n",
    "## model initialization\n",
    "model = CNN(classes = num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:', device)\n",
    "model = model.to(device)\n",
    "\n",
    "## optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "## loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "## data preparation\n",
    "train_loader, valid_loader = load_data(data_dir=data_dir,input_size=inupt_size, batch_size=batch_size)\n",
    "# train\n",
    "train_model(model,train_loader, valid_loader, criterion, optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1debc41d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
